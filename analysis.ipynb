{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Astrocyte Calcium Signaling Analysis Pipeline\n",
    "\n",
    "This notebook processes AQuA2-exported calcium event data from astrocyte recordings,\n",
    "applying feature-specific normalization and performing statistical comparisons\n",
    "across experimental groups.\n",
    "\n",
    "Workflow:\n",
    "    1. Load raw CSV files from AQuA2 output\n",
    "    2. Filter events by frame range (exclude artifacts)\n",
    "    3. Normalize features relative to baseline condition\n",
    "    4. Generate timepoint plots (Baseline -> Drug -> Washout)\n",
    "    5. Perform statistical analysis (Kruskal-Wallis, Dunn's post-hoc, FDR correction)\n",
    "    6. Export normalized data for use in analysis_time.ipynb\n",
    "\n",
    "Experimental Groups:\n",
    "    - WT: Wild Type (control)\n",
    "    - AV: Antagonist Volinanserin (5-HT2A antagonist)\n",
    "    - IP: IP3R2 cKO (calcium signaling knockout)\n",
    "    - CE: CalEx (calcium exchanger manipulation)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.stats import kruskal, rankdata\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Suppress RuntimeWarnings from statistical tests with small samples\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bten8z5lysc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# --- Analysis Parameters ---\n",
    "MIN_FRAME = 20       # Start frame (excludes early recording artifacts)\n",
    "MAX_FRAME = 100      # End frame (excludes late recording artifacts)\n",
    "OUTPUT_DIR = 'Output__'  # Directory containing AQuA2 output\n",
    "CHANNEL_SUFFIX = 'Ch1'   # AQuA2 channel identifier in filenames (set to '' if not used)\n",
    "CWD = os.getcwd()\n",
    "\n",
    "# --- Experimental Group Configuration ---\n",
    "# Each group specifies:\n",
    "#   - path: folder name in OUTPUT_DIR\n",
    "#   - drug_suffix: filename suffix for drug condition (e.g., 'psi', 'psi+antag')\n",
    "#   - slices: dict mapping data subfolders to slice numbers\n",
    "DATA_CONFIG = {\n",
    "    'WT': {\n",
    "        'path': 'WT',\n",
    "        'drug_suffix': 'psi',\n",
    "        'slices': {\n",
    "            'data1': [1, 2, 3],\n",
    "            'data2': [1, 2, 3, 4],\n",
    "        }\n",
    "    },\n",
    "    'AV': {\n",
    "        'path': 'Antagonist- Volinanserin',\n",
    "        'drug_suffix': 'psi+antag',\n",
    "        'slices': {\n",
    "            'data1': [1, 2, 3, 4],\n",
    "            'data2': [1, 2, 3, 4],\n",
    "        }\n",
    "    },\n",
    "    'IP': {\n",
    "        'path': 'IP3R2 cKO',\n",
    "        'drug_suffix': 'psi',\n",
    "        'slices': {\n",
    "            'data1': [2, 4, 5],\n",
    "            'data2': [1, 2, 3],\n",
    "        }\n",
    "    },\n",
    "    'CE': {\n",
    "        'path': 'CalEx',\n",
    "        'drug_suffix': 'psi',\n",
    "        'slices': {\n",
    "            'data1': [1, 2, 3, 4, 5, 6],\n",
    "            'data2': [1, 2, 3],\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE CATEGORIES FOR NORMALIZATION\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Normalization Strategy:\n",
    "    All features are normalized using fold-change relative to baseline median.\n",
    "    \n",
    "    FOLD-CHANGE (value / baseline_median):\n",
    "        - Intensity/magnitude features (dF, dF/F, AUC)\n",
    "        - Spatial features (Area, Perimeter, Circularity)\n",
    "        - Temporal features (durations)\n",
    "        - Network/count features (event counts)\n",
    "        - Interpretation: 2.0 = doubled, 0.5 = halved\n",
    "\"\"\"\n",
    "\n",
    "# Fold-change features: value / baseline_median\n",
    "FOLD_CHANGE_FEATURES = [\n",
    "    \"Curve - Max Df\",\n",
    "    \"Curve - Max Dff\",\n",
    "    \"Curve - dat AUC\",\n",
    "    \"Curve - df AUC\",\n",
    "    \"Curve - dff AUC\",\n",
    "    \"Basic - Area\",\n",
    "    \"Basic - Perimeter (only for 2D video)\",\n",
    "    \"Basic - Circularity\",  \n",
    "    \"Curve - Duration of visualized event overlay\",\n",
    "    \"Curve - Duration 50% to 50% based on averge dF/F\",\n",
    "    \"Curve - Duration 10% to 10% based on averge dF/F\",\n",
    "    \"Curve - Rising duration 10% to 90% based on averge dF/F\",\n",
    "    \"Curve - Decaying duration 90% to 10% based on averge dF/F\",\n",
    "    \"Network - number of events in the same location\",\n",
    "    \"Network - number of events in the same location with similar size only\",\n",
    "    \"Network - maximum number of events appearing at the same time\",\n",
    "]\n",
    "\n",
    "# Combined list of all features to analyze\n",
    "ALL_FEATURES = FOLD_CHANGE_FEATURES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26jri7yahfi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PROCESSING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess a single AQuA2 CSV export file.\n",
    "    \n",
    "    AQuA2 exports data in transposed format (features as rows, events as columns).\n",
    "    This function:\n",
    "        1. Transposes to standard format (events as rows)\n",
    "        2. Drops irrelevant columns (Channel, Index, etc.)\n",
    "        3. Converts all values to numeric\n",
    "        4. Filters events by frame range\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to AQuA2 CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with events as rows and features as columns,\n",
    "        or empty DataFrame if file doesn't exist or has no valid data\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Load and transpose (AQuA2 exports features as rows)\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    df_transposed = df.set_index(0).T.reset_index(drop=True)\n",
    "\n",
    "    # Remove non-feature columns\n",
    "    columns_to_drop = [\n",
    "        'Channel',\n",
    "        'Index',\n",
    "        'Curve - P Value on max Dff (-log10)',\n",
    "        'Curve - Decay tau'\n",
    "    ]\n",
    "    df_clean = df_transposed.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "\n",
    "    # Convert all columns to numeric\n",
    "    for col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "    # Filter by frame range to exclude artifacts\n",
    "    if 'Starting Frame' in df_clean.columns:\n",
    "        df_filtered = df_clean[\n",
    "            (df_clean['Starting Frame'] >= MIN_FRAME) &\n",
    "            (df_clean['Starting Frame'] <= MAX_FRAME)\n",
    "        ].copy()\n",
    "    else:\n",
    "        df_filtered = df_clean.copy()\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def normalize_to_baseline(df, baseline_medians):\n",
    "    \"\"\"\n",
    "    Normalize feature values using fold-change relative to baseline median.\n",
    "    \n",
    "    Formula: normalized_value = value / baseline_median\n",
    "    \n",
    "    Features with zero or NaN baseline medians are set to NaN.\n",
    "    Infinite values (from edge cases) are replaced with NaN.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with raw feature values\n",
    "        baseline_medians: Series of median values from baseline condition\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with fold-change normalized values\n",
    "    \"\"\"\n",
    "    norm_df = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col == \"Starting Frame\":\n",
    "            continue\n",
    "\n",
    "        baseline_val = baseline_medians.get(col, 0)\n",
    "        \n",
    "        if pd.isna(baseline_val) or baseline_val == 0:\n",
    "            norm_df[col] = np.nan\n",
    "        else:\n",
    "            norm_df[col] = df[col] / baseline_val\n",
    "\n",
    "    return norm_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "def analyze_slice(baseline_path, drug_path, washout_path):\n",
    "    \"\"\"\n",
    "    Process and normalize a triplet of condition files for one slice.\n",
    "    \n",
    "    Args:\n",
    "        baseline_path: Path to baseline condition CSV\n",
    "        drug_path: Path to drug/PSI condition CSV\n",
    "        washout_path: Path to washout condition CSV\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (normalized_baseline, normalized_drug, normalized_washout) DataFrames\n",
    "        Returns empty DataFrames if any file is missing or empty\n",
    "    \"\"\"\n",
    "    df_base = process_file(baseline_path)\n",
    "    df_drug = process_file(drug_path)\n",
    "    df_wash = process_file(washout_path)\n",
    "\n",
    "    # All three conditions required for analysis\n",
    "    if df_base.empty or df_drug.empty or df_wash.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Calculate baseline medians for normalization (avoid division by zero)\n",
    "    baseline_medians = df_base.median().replace(0, np.nan)\n",
    "\n",
    "    # Normalize all conditions relative to baseline\n",
    "    norm_base = normalize_to_baseline(df_base, baseline_medians)\n",
    "    norm_drug = normalize_to_baseline(df_drug, baseline_medians)\n",
    "    norm_wash = normalize_to_baseline(df_wash, baseline_medians)\n",
    "\n",
    "    return norm_base, norm_drug, norm_wash\n",
    "\n",
    "\n",
    "def build_file_paths(group_config):\n",
    "    \"\"\"\n",
    "    Generate file path triplets for all slices in an experimental group.\n",
    "    \n",
    "    Args:\n",
    "        group_config: Dict with 'path', 'drug_suffix', and 'slices' keys\n",
    "        \n",
    "    Returns:\n",
    "        List of (baseline_path, drug_path, washout_path) tuples\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    base_folder = group_config['path']\n",
    "    drug_suffix = group_config['drug_suffix']\n",
    "    \n",
    "    # Build channel suffix part of filename\n",
    "    ch = f'_{CHANNEL_SUFFIX}' if CHANNEL_SUFFIX else ''\n",
    "\n",
    "    for data_subfolder, slice_nums in group_config['slices'].items():\n",
    "        for slice_num in slice_nums:\n",
    "            folder = os.path.join(CWD, OUTPUT_DIR, base_folder, data_subfolder)\n",
    "            triplets.append((\n",
    "                os.path.join(folder, f'slice{slice_num}_baseline_AQuA2{ch}.csv'),\n",
    "                os.path.join(folder, f'slice{slice_num}_{drug_suffix}_AQuA2{ch}.csv'),\n",
    "                os.path.join(folder, f'slice{slice_num}_washout_AQuA2{ch}.csv'),\n",
    "            ))\n",
    "    return triplets\n",
    "\n",
    "\n",
    "def process_group(group_name):\n",
    "    \"\"\"\n",
    "    Process all slices for an experimental group and combine results.\n",
    "    \n",
    "    Args:\n",
    "        group_name: Key in DATA_CONFIG (e.g., 'WT', 'AV', 'IP', 'CE')\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (combined_baseline, combined_drug, combined_washout) DataFrames\n",
    "        with data from all slices concatenated\n",
    "    \"\"\"\n",
    "    print(f\"Processing Group: {group_name}...\")\n",
    "    config = DATA_CONFIG[group_name]\n",
    "    triplets = build_file_paths(config)\n",
    "\n",
    "    base_list, drug_list, wash_list = [], [], []\n",
    "\n",
    "    for baseline_path, drug_path, washout_path in triplets:\n",
    "        if os.path.exists(baseline_path):\n",
    "            norm_base, norm_drug, norm_wash = analyze_slice(\n",
    "                baseline_path, drug_path, washout_path\n",
    "            )\n",
    "            if not norm_base.empty:\n",
    "                base_list.append(norm_base)\n",
    "                drug_list.append(norm_drug)\n",
    "                wash_list.append(norm_wash)\n",
    "        else:\n",
    "            print(f\"  Missing: {baseline_path}\")\n",
    "\n",
    "    # Combine all slices\n",
    "    if base_list:\n",
    "        return (\n",
    "            pd.concat(base_list, ignore_index=True),\n",
    "            pd.concat(drug_list, ignore_index=True),\n",
    "            pd.concat(wash_list, ignore_index=True)\n",
    "        )\n",
    "    return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "\n",
    "def save_normalized_data(base_df, drug_df, wash_df, group_name):\n",
    "    \"\"\"\n",
    "    Save normalized DataFrames to CSV for later use.\n",
    "    \n",
    "    Files are saved to: OUTPUT_DIR/<group_path>/<group>_<condition>_normalized.csv\n",
    "    \n",
    "    Args:\n",
    "        base_df: Normalized baseline DataFrame\n",
    "        drug_df: Normalized drug condition DataFrame\n",
    "        wash_df: Normalized washout DataFrame\n",
    "        group_name: Group identifier (e.g., 'WT')\n",
    "    \"\"\"\n",
    "    folder_name = DATA_CONFIG[group_name]['path']\n",
    "    output_path = os.path.join(CWD, OUTPUT_DIR, folder_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    base_df.to_csv(os.path.join(output_path, f'{group_name}_baseline_normalized.csv'), index=False)\n",
    "    drug_df.to_csv(os.path.join(output_path, f'{group_name}_drug_normalized.csv'), index=False)\n",
    "    wash_df.to_csv(os.path.join(output_path, f'{group_name}_washout_normalized.csv'), index=False)\n",
    "    \n",
    "    print(f\"  Saved normalized data to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gueixjmhv1g",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STATISTICAL ANALYSIS FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def dunns_test(groups, labels, control=None):\n",
    "    \"\"\"\n",
    "    Perform Dunn's post-hoc test for pairwise comparisons after Kruskal-Wallis.\n",
    "    \n",
    "    Dunn's test compares groups using rank sums from the combined sample,\n",
    "    with a z-test for each pair. This is the appropriate non-parametric\n",
    "    post-hoc test following a significant Kruskal-Wallis result.\n",
    "    \n",
    "    Args:\n",
    "        groups: List of arrays, one per group\n",
    "        labels: List of group labels corresponding to each array\n",
    "        control: If specified, only compare other groups against this control group.\n",
    "                 If None, perform all pairwise comparisons.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: Group1, Group2, Z, p_value, p_adjusted\n",
    "        p_adjusted uses Benjamini-Hochberg FDR correction\n",
    "    \"\"\"\n",
    "    # Combine all data and rank\n",
    "    all_data = np.concatenate(groups)\n",
    "    N = len(all_data)\n",
    "    ranks = rankdata(all_data)\n",
    "    \n",
    "    # Calculate mean rank per group\n",
    "    group_sizes = [len(g) for g in groups]\n",
    "    mean_ranks = []\n",
    "    start = 0\n",
    "    for size in group_sizes:\n",
    "        mean_ranks.append(np.mean(ranks[start:start + size]))\n",
    "        start += size\n",
    "    \n",
    "    # Tie correction factor\n",
    "    _, tie_counts = np.unique(ranks, return_counts=True)\n",
    "    tie_correction = 1 - np.sum(tie_counts**3 - tie_counts) / (N**3 - N)\n",
    "    \n",
    "    # Avoid division by zero if all values are tied\n",
    "    if tie_correction == 0:\n",
    "        tie_correction = 1.0\n",
    "    \n",
    "    # Pairwise comparisons\n",
    "    results = []\n",
    "    \n",
    "    if control is not None and control in labels:\n",
    "        # Only compare against control\n",
    "        ctrl_idx = labels.index(control)\n",
    "        pairs = [(ctrl_idx, j) for j in range(len(labels)) if j != ctrl_idx]\n",
    "    else:\n",
    "        # All pairwise comparisons\n",
    "        pairs = [(i, j) for i in range(len(labels)) for j in range(i+1, len(labels))]\n",
    "    \n",
    "    for i, j in pairs:\n",
    "        n_i, n_j = group_sizes[i], group_sizes[j]\n",
    "        \n",
    "        # Standard error for the difference in mean ranks\n",
    "        se = np.sqrt((N * (N + 1) / 12.0) * (1.0/n_i + 1.0/n_j) / tie_correction)\n",
    "        \n",
    "        if se == 0:\n",
    "            z_stat = 0.0\n",
    "            p_val = 1.0\n",
    "        else:\n",
    "            z_stat = (mean_ranks[i] - mean_ranks[j]) / se\n",
    "            p_val = 2.0 * stats.norm.sf(abs(z_stat))  # Two-tailed\n",
    "        \n",
    "        results.append({\n",
    "            'Group1': labels[i],\n",
    "            'Group2': labels[j],\n",
    "            'Z': z_stat,\n",
    "            'p_value': p_val\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # FDR correction on pairwise p-values\n",
    "    if len(results_df) > 0:\n",
    "        _, results_df['p_adjusted'], _, _ = multipletests(\n",
    "            results_df['p_value'], method='fdr_bh'\n",
    "        )\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def perform_statistical_tests(feature_name, group_data_dict, control_group='WT'):\n",
    "    \"\"\"\n",
    "    Perform Kruskal-Wallis test with Dunn's post-hoc for one feature.\n",
    "    \n",
    "    Statistical approach:\n",
    "        1. Kruskal-Wallis: Non-parametric omnibus test across all groups\n",
    "           (appropriate for fold-change data which is typically right-skewed)\n",
    "        2. Dunn's test: Pairwise comparisons against control group\n",
    "           (only if Kruskal-Wallis is significant after FDR correction)\n",
    "    \n",
    "    Args:\n",
    "        feature_name: Column name to analyze\n",
    "        group_data_dict: Dict mapping group names to DataFrames\n",
    "        control_group: Reference group for pairwise comparisons (default: 'WT')\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing test statistics, p-values, medians, and sample sizes\n",
    "    \"\"\"\n",
    "    arrays = []\n",
    "    labels = []\n",
    "    group_keys = list(group_data_dict.keys())\n",
    "\n",
    "    # Extract data for each group\n",
    "    for name in group_keys:\n",
    "        df = group_data_dict[name]\n",
    "        if feature_name in df.columns:\n",
    "            arr = df[feature_name].dropna().values\n",
    "            arrays.append(arr)\n",
    "            labels.append(name)\n",
    "        else:\n",
    "            arrays.append(np.array([]))\n",
    "            labels.append(name)\n",
    "\n",
    "    # Need at least 2 groups with >1 observation for comparison\n",
    "    valid_indices = [i for i, arr in enumerate(arrays) if len(arr) > 1]\n",
    "\n",
    "    if len(valid_indices) < 2:\n",
    "        return {\n",
    "            'feature': feature_name,\n",
    "            'kruskal_h': np.nan,\n",
    "            'kruskal_p': np.nan,\n",
    "            'dunn_results': None,\n",
    "            'group_keys': [],\n",
    "            'note': 'Insufficient data'\n",
    "        }\n",
    "\n",
    "    # Filter to valid groups only\n",
    "    valid_arrays = [arrays[i] for i in valid_indices]\n",
    "    valid_labels = [labels[i] for i in valid_indices]\n",
    "\n",
    "    # Kruskal-Wallis omnibus test\n",
    "    kruskal_h, kruskal_p = kruskal(*valid_arrays)\n",
    "\n",
    "    # Build results dictionary\n",
    "    result = {\n",
    "        'feature': feature_name,\n",
    "        'kruskal_h': kruskal_h,\n",
    "        'kruskal_p': kruskal_p,\n",
    "        'dunn_results': None,\n",
    "        'group_keys': valid_labels\n",
    "    }\n",
    "\n",
    "    # Add per-group statistics\n",
    "    for i, name in enumerate(labels):\n",
    "        arr = arrays[i]\n",
    "        result[f'median_{name}'] = np.median(arr) if len(arr) > 0 else np.nan\n",
    "        result[f'n_{name}'] = len(arr)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def analyze_all_features(group_dict, features_list, condition_name=\"Drug\", \n",
    "                         control_group='WT', alpha=0.05):\n",
    "    \"\"\"\n",
    "    Run Kruskal-Wallis on all features, apply FDR, then run Dunn's post-hoc.\n",
    "    \n",
    "    Workflow:\n",
    "        1. Run Kruskal-Wallis for each feature\n",
    "        2. Apply FDR correction across all features\n",
    "        3. For features significant after FDR correction, run Dunn's post-hoc\n",
    "    \n",
    "    Args:\n",
    "        group_dict: Dict mapping group names to DataFrames\n",
    "        features_list: List of feature column names to analyze\n",
    "        condition_name: Label for output (e.g., \"Drug\", \"Washout\")\n",
    "        control_group: Reference group for Dunn's pairwise comparisons\n",
    "        alpha: Significance threshold (default 0.05)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with test results for all features, including FDR-corrected p-values\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Statistical Analysis: {condition_name} Condition\")\n",
    "    print(f\"Method: Kruskal-Wallis + Dunn's post-hoc (FDR corrected)\")\n",
    "    print(f\"Comparing groups: {', '.join(group_dict.keys())}\")\n",
    "    print(f\"Control group: {control_group}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "    # Step 1: Run Kruskal-Wallis for each feature\n",
    "    results = []\n",
    "    for feature in features_list:\n",
    "        res = perform_statistical_tests(feature, group_dict, control_group)\n",
    "        results.append(res)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Step 2: Apply FDR correction to Kruskal-Wallis p-values\n",
    "    if not results_df.empty and 'kruskal_p' in results_df.columns:\n",
    "        valid_p = results_df['kruskal_p'].dropna()\n",
    "        if len(valid_p) > 0:\n",
    "            _, results_df['kruskal_p_fdr'], _, _ = multipletests(\n",
    "                results_df['kruskal_p'].fillna(1.0), method='fdr_bh'\n",
    "            )\n",
    "\n",
    "    # Step 3: Run Dunn's post-hoc ONLY for features significant after FDR\n",
    "    if 'kruskal_p_fdr' in results_df.columns:\n",
    "        for idx, row in results_df.iterrows():\n",
    "            if pd.notna(row['kruskal_p_fdr']) and row['kruskal_p_fdr'] < alpha:\n",
    "                valid_groups = row.get('group_keys', [])\n",
    "                if len(valid_groups) >= 2:\n",
    "                    # Get arrays for valid groups\n",
    "                    arrays = []\n",
    "                    labels = []\n",
    "                    for name in valid_groups:\n",
    "                        df = group_dict[name]\n",
    "                        feature = row['feature']\n",
    "                        if feature in df.columns:\n",
    "                            arr = df[feature].dropna().values\n",
    "                            if len(arr) > 1:\n",
    "                                arrays.append(arr)\n",
    "                                labels.append(name)\n",
    "                    \n",
    "                    if len(arrays) >= 2:\n",
    "                        dunn_df = dunns_test(arrays, labels, control=control_group)\n",
    "                        results_df.at[idx, 'dunn_results'] = dunn_df\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def get_significance_stars(p_value):\n",
    "    \"\"\"Convert p-value to significance stars notation.\"\"\"\n",
    "    if pd.isna(p_value):\n",
    "        return \"N/A\"\n",
    "    if p_value < 0.001:\n",
    "        return \"***\"\n",
    "    if p_value < 0.01:\n",
    "        return \"**\"\n",
    "    if p_value < 0.05:\n",
    "        return \"*\"\n",
    "    return \"ns\"\n",
    "\n",
    "\n",
    "def print_results_summary(results_df):\n",
    "    \"\"\"\n",
    "    Print a formatted table of Kruskal-Wallis test results.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame from analyze_all_features()\n",
    "    \"\"\"\n",
    "    if results_df.empty:\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"KRUSKAL-WALLIS TEST RESULTS (FDR corrected)\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Get group names for header\n",
    "    first_row = results_df.iloc[0]\n",
    "    group_keys = first_row.get('group_keys', [])\n",
    "\n",
    "    # Print header\n",
    "    median_header = \"Medians (\" + \"/\".join(group_keys) + \")\"\n",
    "    n_header = \"N (\" + \"/\".join(group_keys) + \")\"\n",
    "    print(f\"{'Feature':<50} {'H stat':<10} {'p (FDR)':<15} {'Sig':<5} {median_header:<30} {n_header}\")\n",
    "    print(\"-\" * 140)\n",
    "\n",
    "    # Print each feature's results\n",
    "    for _, row in results_df.iterrows():\n",
    "        feature = row['feature']\n",
    "\n",
    "        # Kruskal-Wallis results\n",
    "        h_stat = row.get('kruskal_h', np.nan)\n",
    "        kruskal_p = row.get('kruskal_p_fdr', row.get('kruskal_p', np.nan))\n",
    "        \n",
    "        if pd.isna(kruskal_p):\n",
    "            p_str = \"N/A\"\n",
    "            sig = \"N/A\"\n",
    "        else:\n",
    "            p_str = f\"{kruskal_p:.4f}\"\n",
    "            sig = get_significance_stars(kruskal_p)\n",
    "        \n",
    "        h_str = f\"{h_stat:.2f}\" if pd.notna(h_stat) else \"N/A\"\n",
    "\n",
    "        # Group medians and sample sizes\n",
    "        medians_list = []\n",
    "        n_list = []\n",
    "        for g in group_keys:\n",
    "            val = row.get(f'median_{g}', np.nan)\n",
    "            n_val = row.get(f'n_{g}', 0)\n",
    "            medians_list.append(\"-\" if pd.isna(val) else f\"{val:.2f}\")\n",
    "            n_list.append(str(int(n_val)))\n",
    "        median_str = \"/\".join(medians_list)\n",
    "        n_str = \"/\".join(n_list)\n",
    "\n",
    "        # Truncate long feature names\n",
    "        feature_short = feature[:47] + \"...\" if len(feature) > 50 else feature\n",
    "        print(f\"{feature_short:<50} {h_str:<10} {p_str:<15} {sig:<5} {median_str:<30} {n_str}\")\n",
    "\n",
    "    print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "\n",
    "def print_dunn_results(results_df, control_group='WT'):\n",
    "    \"\"\"\n",
    "    Print Dunn's post-hoc test results for significant features.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame from analyze_all_features()\n",
    "        control_group: Reference group for pairwise comparisons\n",
    "    \"\"\"\n",
    "    # Filter to features with Dunn's results\n",
    "    has_dunn = results_df[results_df['dunn_results'].apply(\n",
    "        lambda x: x is not None and isinstance(x, pd.DataFrame) and len(x) > 0\n",
    "    )]\n",
    "\n",
    "    if len(has_dunn) == 0:\n",
    "        print(\"\\nNo significant features found for post-hoc testing.\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DUNN'S POST-HOC TEST RESULTS (FDR corrected)\")\n",
    "    print(f\"Pairwise comparisons against control: {control_group}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for _, row in has_dunn.iterrows():\n",
    "        dunn_df = row['dunn_results']\n",
    "        valid_groups = row.get('group_keys', [])\n",
    "        \n",
    "        print(f\"\\n{row['feature']}:\")\n",
    "        \n",
    "        # Print group medians\n",
    "        medians = [f\"{g}={row.get(f'median_{g}', np.nan):.2f}\" for g in valid_groups]\n",
    "        print(f\"  Group medians: {', '.join(medians)}\")\n",
    "        print(f\"  Pairwise comparisons (vs {control_group}):\")\n",
    "        \n",
    "        for _, comp in dunn_df.iterrows():\n",
    "            sig = get_significance_stars(comp['p_adjusted'])\n",
    "            print(f\"    {comp['Group1']} vs {comp['Group2']}: \"\n",
    "                  f\"Z={comp['Z']:.3f}, p={comp['p_adjusted']:.4f} {sig}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zb0voqckikd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_mad(series):\n",
    "    \"\"\"\n",
    "    Calculate Median Absolute Deviation (MAD).\n",
    "    \n",
    "    MAD is a robust measure of variability, less sensitive to outliers than\n",
    "    standard deviation. Calculated as: median(|x - median(x)|)\n",
    "    \n",
    "    Args:\n",
    "        series: Pandas Series of numeric values\n",
    "        \n",
    "    Returns:\n",
    "        MAD value (float)\n",
    "    \"\"\"\n",
    "    return (series - series.median()).abs().median()\n",
    "\n",
    "\n",
    "def calculate_iqr_half(series):\n",
    "    \"\"\"\n",
    "    Calculate half Interquartile Range (IQR/2) for symmetric error bars.\n",
    "    \n",
    "    Returns (Q3-Q1)/2 for use as symmetric error around the median.\n",
    "    \n",
    "    Args:\n",
    "        series: Pandas Series of numeric values\n",
    "        \n",
    "    Returns:\n",
    "        Half-IQR value (float)\n",
    "    \"\"\"\n",
    "    q75, q25 = np.percentile(series.dropna(), [75, 25])\n",
    "    return (q75 - q25) / 2\n",
    "\n",
    "\n",
    "def plot_timepoints(base_df, drug_df, wash_df, features_to_plot, title=\"Analysis\", \n",
    "                    error_type='iqr'):\n",
    "    \"\"\"\n",
    "    Plot feature values across experimental timepoints (Baseline -> Drug -> Washout).\n",
    "    \n",
    "    Creates a multi-panel figure showing median values (Â± error) for each feature\n",
    "    across the three experimental conditions.\n",
    "    \n",
    "    Args:\n",
    "        base_df: Normalized baseline condition DataFrame\n",
    "        drug_df: Normalized drug condition DataFrame\n",
    "        wash_df: Normalized washout DataFrame\n",
    "        features_to_plot: List of feature column names to include\n",
    "        title: Figure title\n",
    "        error_type: Type of error bars (default: 'iqr')\n",
    "            - 'mad': Median Absolute Deviation (robust, pairs with median)\n",
    "            - 'iqr': Half Interquartile Range (robust, good for skewed data)\n",
    "    \"\"\"\n",
    "    # Select error calculation function\n",
    "    error_functions = {\n",
    "        'mad': ('MAD', calculate_mad),\n",
    "        'iqr': ('IQR/2', calculate_iqr_half),\n",
    "    }\n",
    "    error_label, error_func = error_functions.get(error_type, ('IQR/2', calculate_iqr_half))\n",
    "    \n",
    "    # Organize data by condition\n",
    "    conditions = {'BASELINE': base_df, 'DRUG': drug_df, 'WASHOUT': wash_df}\n",
    "    medians_dict = {}\n",
    "    errors_dict = {}\n",
    "\n",
    "    # Calculate statistics for each condition\n",
    "    for name, df in conditions.items():\n",
    "        if df.empty:\n",
    "            continue\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "        medians_dict[name] = df[numeric_cols].median()\n",
    "        errors_dict[name] = df[numeric_cols].apply(error_func)\n",
    "\n",
    "    if not medians_dict:\n",
    "        print(f\"No data available for {title}\")\n",
    "        return\n",
    "\n",
    "    # Combine into DataFrames for easy plotting\n",
    "    combined_medians = pd.DataFrame(medians_dict)\n",
    "    combined_errors = pd.DataFrame(errors_dict)\n",
    "\n",
    "    # Filter to available features\n",
    "    available_features = [f for f in features_to_plot if f in combined_medians.index]\n",
    "    if not available_features:\n",
    "        print(f\"No features available for {title}\")\n",
    "        return\n",
    "\n",
    "    # Setup figure grid\n",
    "    n_cols = 4\n",
    "    n_rows = (len(available_features) + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 3.5), \n",
    "                              constrained_layout=True)\n",
    "    \n",
    "    fig.suptitle(f\"{title} (Median \\u00B1 {error_label})\", fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Handle single subplot case\n",
    "    if len(available_features) == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    # Plot each feature\n",
    "    for i, feature in enumerate(available_features):\n",
    "        ax = axes[i]\n",
    "        medians = combined_medians.loc[feature]\n",
    "        errors = combined_errors.loc[feature]\n",
    "        x_pos = range(len(medians))\n",
    "\n",
    "        # Plot median points\n",
    "        ax.plot(x_pos, medians, 'o', color='black', markersize=8, label='Median')\n",
    "\n",
    "        # Add error bars\n",
    "        for j, pos in enumerate(x_pos):\n",
    "            ax.vlines(pos, medians.iloc[j] - errors.iloc[j], \n",
    "                     medians.iloc[j] + errors.iloc[j], color='grey', lw=2)\n",
    "\n",
    "        ax.set_ylabel('Fold Change')\n",
    "        ax.set_title(feature, fontsize=9)\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(medians.index)\n",
    "        ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for j in range(len(available_features), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_group_comparison(group_dict, feature_name, condition=\"Drug\", error_type='iqr'):\n",
    "    \"\"\"\n",
    "    Create a bar chart comparing one feature across all experimental groups.\n",
    "    \n",
    "    Args:\n",
    "        group_dict: Dict mapping group names to DataFrames\n",
    "        feature_name: Column name to plot\n",
    "        condition: Label for the condition (used in title)\n",
    "        error_type: Type of error bars - 'mad' or 'iqr' (default: 'iqr')\n",
    "    \"\"\"\n",
    "    error_functions = {\n",
    "        'mad': ('MAD', calculate_mad),\n",
    "        'iqr': ('IQR/2', calculate_iqr_half),\n",
    "    }\n",
    "    error_label, error_func = error_functions.get(error_type, ('IQR/2', calculate_iqr_half))\n",
    "    \n",
    "    groups = list(group_dict.keys())\n",
    "    medians = []\n",
    "    errors = []\n",
    "\n",
    "    for group in groups:\n",
    "        df = group_dict[group]\n",
    "        if feature_name in df.columns and not df.empty:\n",
    "            med = df[feature_name].median()\n",
    "            err = error_func(df[feature_name])\n",
    "            medians.append(med)\n",
    "            errors.append(err)\n",
    "        else:\n",
    "            medians.append(0)\n",
    "            errors.append(0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    x_pos = np.arange(len(groups))\n",
    "\n",
    "    bars = ax.bar(x_pos, medians, yerr=errors, align='center', \n",
    "                  alpha=0.7, ecolor='black', capsize=10)\n",
    "\n",
    "    for i, group in enumerate(groups):\n",
    "        bars[i].set_color('gray' if group == 'WT' else 'skyblue')\n",
    "\n",
    "    ax.set_ylabel(feature_name)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(groups)\n",
    "    ax.set_title(f'{condition} Condition: {feature_name}\\n(Median \\u00B1 {error_label})')\n",
    "    ax.yaxis.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hxprzqoev6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION: DATA PROCESSING\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Process all experimental groups:\n",
    "    1. Load raw AQuA2 CSV files for each slice\n",
    "    2. Apply fold-change normalization relative to baseline\n",
    "    3. Combine data across slices\n",
    "    4. Save normalized data for downstream analysis\n",
    "\"\"\"\n",
    "\n",
    "# Storage for processed data\n",
    "group_data = {\n",
    "    'baseline': {},\n",
    "    'drug': {},\n",
    "    'washout': {}\n",
    "}\n",
    "\n",
    "# Process each experimental group\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROCESSING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for group_name in DATA_CONFIG.keys():\n",
    "    base_df, drug_df, wash_df = process_group(group_name)\n",
    "    \n",
    "    group_data['baseline'][group_name] = base_df\n",
    "    group_data['drug'][group_name] = drug_df\n",
    "    group_data['washout'][group_name] = wash_df\n",
    "    \n",
    "    # Save normalized data for use in analysis_time.ipynb\n",
    "    if not base_df.empty:\n",
    "        save_normalized_data(base_df, drug_df, wash_df, group_name)\n",
    "    else:\n",
    "        print(f\"  Warning: No valid data for {group_name}\")\n",
    "\n",
    "print(\"\\nData processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qeqn1s3a4u",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION: VISUALIZATION\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Generate timepoint plots showing Baseline -> Drug -> Washout transitions\n",
    "for each experimental group.\n",
    "\n",
    "Error bar options:\n",
    "    - 'iqr': Half Interquartile Range (robust, good for skewed fold-change data)\n",
    "    - 'mad': Median Absolute Deviation (robust alternative)\n",
    "\"\"\"\n",
    "\n",
    "# Choose error type: 'iqr' or 'mad'\n",
    "ERROR_TYPE = 'iqr'\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING TIMEPOINT PLOTS\")\n",
    "print(f\"Error bars: {ERROR_TYPE.upper()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for group_name in DATA_CONFIG.keys():\n",
    "    base_df = group_data['baseline'][group_name]\n",
    "    drug_df = group_data['drug'][group_name]\n",
    "    wash_df = group_data['washout'][group_name]\n",
    "    \n",
    "    if not drug_df.empty:\n",
    "        print(f\"\\nPlotting: {group_name}\")\n",
    "        plot_timepoints(\n",
    "            base_df, drug_df, wash_df,\n",
    "            ALL_FEATURES,\n",
    "            title=f'{group_name} Analysis',\n",
    "            error_type=ERROR_TYPE\n",
    "        )\n",
    "    else:\n",
    "        print(f\"\\nSkipping {group_name}: No data available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soai2k621s7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION: STATISTICAL ANALYSIS\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Perform statistical comparisons across experimental groups:\n",
    "    - Kruskal-Wallis (non-parametric omnibus test)\n",
    "    - Dunn's post-hoc (pairwise comparisons vs WT control)\n",
    "    - FDR correction (Benjamini-Hochberg) at both omnibus and post-hoc levels\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- Analyze Drug Condition ---\n",
    "print(\"\\n>>> Analyzing DRUG condition...\")\n",
    "drug_results = analyze_all_features(\n",
    "    group_data['drug'],\n",
    "    ALL_FEATURES,\n",
    "    condition_name=\"Drug\",\n",
    "    control_group='WT'\n",
    ")\n",
    "print_results_summary(drug_results)\n",
    "print_dunn_results(drug_results, control_group='WT')\n",
    "\n",
    "# --- Analyze Washout Condition ---\n",
    "print(\"\\n>>> Analyzing WASHOUT condition...\")\n",
    "washout_results = analyze_all_features(\n",
    "    group_data['washout'],\n",
    "    ALL_FEATURES,\n",
    "    condition_name=\"Washout\",\n",
    "    control_group='WT'\n",
    ")\n",
    "print_results_summary(washout_results)\n",
    "print_dunn_results(washout_results, control_group='WT')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Astrocytes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
